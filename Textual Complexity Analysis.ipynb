{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize,word_tokenize \n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook Compute the complexity of the proposed/final regulations and comments\n",
    "\n",
    "Input is text files orginized in folders\n",
    "\n",
    "Result is in a diction format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_complexity(text):\n",
    "    num_chars=len(text)\n",
    "    num_words=len(word_tokenize(text))\n",
    "    num_sentences=len(sent_tokenize(text))\n",
    "    vocab = {x.lower() for x in word_tokenize(text)}\n",
    "    try:\n",
    "        return len(vocab),int(num_chars/num_words),int(num_words/num_sentences),len(vocab)/num_words\n",
    "    except:\n",
    "        return 0, 0, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './OCC'\n",
    "# input folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\") or filename.endswith(\".png\"):\n",
    "        print(os.path.join(directory, filename))\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Function\n",
    "\n",
    "result in res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./OCC/OCC-2018-0038\n",
      "./OCC/OCC-2014-0008\n",
      "./OCC/OCC-2017-0018\n",
      "./OCC/OCC-2017-0011\n",
      "./OCC/OCC-2018-0009\n",
      "./OCC/OCC-2014-0001\n",
      "./OCC/OCC-2013-0013\n",
      "./OCC/OCC-2018-0030\n",
      "./OCC/CFPB-2012-0031\n",
      "./OCC/OCC-2014-0007\n",
      "./OCC/OCC-2018-0008\n",
      "./OCC/OCC-2014-0009\n",
      "./OCC/OCC-2017-0021\n",
      "./OCC/OCC-2018-0039\n",
      "./OCC/OCC-2019-0001\n",
      "./OCC/OCC-2016-0017\n",
      "./OCC/OCC-2012-0013\n",
      "./OCC/OCC-2019-0009\n",
      "./OCC/OCC-2016-0002\n",
      "./OCC/OCC-2012-0008\n",
      "./OCC/OCC-2019-0023\n",
      "./OCC/OCC-2020-0002\n",
      "./OCC/OCC-2018-0041\n",
      "./OCC/OCC-2013-0008\n",
      "./OCC/OCC-2014-0025\n",
      "./OCC/OCC-2018-0040\n",
      "./OCC/OCC-2013-0009\n",
      "./OCC/OCC-2014-0012\n",
      "./OCC/OCC-2018-0003\n",
      "./OCC/OCC-2013-0010\n",
      "./OCC/OCC-2017-0012\n",
      "./OCC/OCC-2014-0002\n",
      "./OCC/OCC-2017-0013\n",
      "./OCC/OCC-2013-0016\n",
      "./OCC/OCC-2011-0008\n",
      "./OCC/OCC-2016-0022\n",
      "./OCC/OCC-2019-0004\n",
      "./OCC/CFPB-2013-0020\n",
      "./OCC/OCC-2015-0021\n",
      "./OCC/OCC-2011-0023\n",
      "./OCC/OCC-2016-0009\n",
      "./OCC/OCC-2015-0017\n",
      "./OCC/OCC-2018-0020\n",
      "./OCC/OCC-2018-0011\n",
      "./OCC/OCC-2018-0029\n",
      "./OCC/OCC-2017-0008\n",
      "./OCC/OCC-2018-0010\n",
      "./OCC/OCC-2018-0026\n",
      "./OCC/OCC-2014-0016\n"
     ]
    }
   ],
   "source": [
    "res_dict = {}\n",
    "evaluate_cate = ['num of vocab', 'word complexity', 'sentence complexity', 'unique words pct']\n",
    "for entry in os.scandir(directory):\n",
    "    if (entry.path.endswith('.DS_Store')):\n",
    "        continue\n",
    "            \n",
    "    print(entry.path)\n",
    "    if not entry.path.endswith('.DS_Store'):\n",
    "        for subentry in os.scandir(entry.path):\n",
    "            if (subentry.path.endswith(\"Links.csv\") or \n",
    "                subentry.path.endswith(\"Links.txt\") or \n",
    "                subentry.path.endswith('.DS_Store')):\n",
    "                continue\n",
    "            \n",
    "            if subentry.path.endswith('Rule'):\n",
    "                docket_id = subentry.path.split('/')[2]\n",
    "                file_type = subentry.path.split('/')[3]\n",
    "\n",
    "                if docket_id not in res_dict:\n",
    "                    res_dict[docket_id] = {}\n",
    "                if file_type not in res_dict[docket_id]:\n",
    "                    res_dict[docket_id][file_type] = {}\n",
    "\n",
    "                if evaluate_cate[0] not in res_dict[docket_id][file_type] and file_type == 'Rule':\n",
    "                    for i in range(len(evaluate_cate)):\n",
    "                        res_dict[docket_id][file_type][evaluate_cate[i]] = []\n",
    "\n",
    "                for filename in os.listdir(subentry):\n",
    "                    if filename.endswith(\".txt\") and not filename.endswith(\"Links.txt\"):\n",
    "                        path_rule = os.path.join(subentry, filename)\n",
    "                        f = open(path_rule, \"r\", encoding=\"ISO-8859-1\")\n",
    "                        text = f.read()\n",
    "                        num_of_vocab, word_complexity, sentence_complexity, unique_words_pct = get_complexity(text)\n",
    "                        res_dict[docket_id][file_type]['num of vocab'].append(num_of_vocab)\n",
    "                        res_dict[docket_id][file_type]['word complexity'].append(word_complexity)\n",
    "                        res_dict[docket_id][file_type]['sentence complexity'].append(sentence_complexity)\n",
    "                        res_dict[docket_id][file_type]['unique words pct'].append(unique_words_pct)\n",
    "                    else:\n",
    "                        continue\n",
    "                        \n",
    "            if subentry.path.endswith('Proposed Rule'):\n",
    "                    \n",
    "                for sub in os.scandir(subentry.path):\n",
    "                    if (sub.path.endswith(\"Links.csv\") or \n",
    "                        sub.path.endswith(\"Links.txt\") or \n",
    "                        sub.path.endswith('.DS_Store')):\n",
    "                        continue\n",
    "                    docket_id = sub.path.split('/')[2]\n",
    "                    file_type = sub.path.split('/')[3]\n",
    "                    pro_rule_id = sub.path.split('/')[4]\n",
    "\n",
    "                    if docket_id not in res_dict:\n",
    "                        res_dict[docket_id] = {}\n",
    "                    if file_type not in res_dict[docket_id]:\n",
    "                        res_dict[docket_id][file_type] = {}\n",
    "                    if pro_rule_id not in res_dict[docket_id][file_type]:\n",
    "                        res_dict[docket_id][file_type][pro_rule_id] = {}\n",
    "                        \n",
    "                    if evaluate_cate[0] not in res_dict[docket_id][file_type][pro_rule_id]:\n",
    "                        for i in range(len(evaluate_cate)):\n",
    "                            res_dict[docket_id][file_type][pro_rule_id][evaluate_cate[i]] = []\n",
    "                            \n",
    "\n",
    "                    for pro_rule_entry in os.scandir(sub.path):\n",
    "#                         if pro_rule_entry.path == './CFPB/CFPB-2011-0023/Proposed Rule/CFPB-2011-0023-0001_content.pdf':\n",
    "#                             continue\n",
    "                        if (pro_rule_entry.path.endswith(\"Links.csv\") or \n",
    "                        pro_rule_entry.path.endswith(\"Links.txt\") or \n",
    "                        pro_rule_entry.path.endswith('.DS_Store')):\n",
    "                            continue\n",
    "                        \n",
    "                        if pro_rule_entry.path.endswith('txt'):\n",
    "                        \n",
    "                            path = pro_rule_entry.path\n",
    "                            f = open(path, \"r\", encoding=\"ISO-8859-1\")\n",
    "                            text = f.read()\n",
    "                            num_of_vocab, word_complexity, sentence_complexity, unique_words_pct = get_complexity(text)\n",
    "                            res_dict[docket_id][file_type][pro_rule_id]['num of vocab'].append(num_of_vocab)\n",
    "                            res_dict[docket_id][file_type][pro_rule_id]['word complexity'].append(word_complexity)\n",
    "                            res_dict[docket_id][file_type][pro_rule_id]['sentence complexity'].append(sentence_complexity)\n",
    "                            res_dict[docket_id][file_type][pro_rule_id]['unique words pct'].append(unique_words_pct)\n",
    "                        \n",
    "                        if pro_rule_entry.path.endswith('comment'):\n",
    "                            if 'comment' not in res_dict[docket_id][file_type]:\n",
    "                                res_dict[docket_id][file_type][pro_rule_id]['comment'] = {}\n",
    "                                \n",
    "                            if evaluate_cate[0] not in res_dict[docket_id][file_type][pro_rule_id]['comment']:\n",
    "                                for i in range(len(evaluate_cate)):\n",
    "                                    res_dict[docket_id][file_type][pro_rule_id]['comment'][evaluate_cate[i]] = []\n",
    "\n",
    "                                \n",
    "                            for filename_path in os.scandir(pro_rule_entry.path):\n",
    "                                filename = filename_path.path\n",
    "                                if filename.endswith(\".txt\") and not filename.endswith(\"Links.txt\"):\n",
    "                                    f = open(filename, \"r\", encoding=\"ISO-8859-1\")\n",
    "                                    text = f.read()\n",
    "                                    num_of_vocab, word_complexity, sentence_complexity, unique_words_pct = get_complexity(text)\n",
    "                                    res_dict[docket_id][file_type][pro_rule_id]['comment']['num of vocab'].append(num_of_vocab)\n",
    "                                    res_dict[docket_id][file_type][pro_rule_id]['comment']['word complexity'].append(word_complexity)\n",
    "                                    res_dict[docket_id][file_type][pro_rule_id]['comment']['sentence complexity'].append(sentence_complexity)\n",
    "                                    res_dict[docket_id][file_type][pro_rule_id]['comment']['unique words pct'].append(unique_words_pct)\n",
    "                    else:\n",
    "                        continue\n",
    "                                          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict_OCC = res_dict.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num of vocab': [135,\n",
       "  40,\n",
       "  168,\n",
       "  355,\n",
       "  808,\n",
       "  219,\n",
       "  117,\n",
       "  231,\n",
       "  128,\n",
       "  131,\n",
       "  99,\n",
       "  442,\n",
       "  167,\n",
       "  0,\n",
       "  233,\n",
       "  63,\n",
       "  204,\n",
       "  1428,\n",
       "  112,\n",
       "  978,\n",
       "  901,\n",
       "  285,\n",
       "  277,\n",
       "  230,\n",
       "  141,\n",
       "  106,\n",
       "  30,\n",
       "  26,\n",
       "  388,\n",
       "  156,\n",
       "  68,\n",
       "  431,\n",
       "  242,\n",
       "  72,\n",
       "  825,\n",
       "  325,\n",
       "  241,\n",
       "  107,\n",
       "  180,\n",
       "  116,\n",
       "  453,\n",
       "  199,\n",
       "  334,\n",
       "  321,\n",
       "  483,\n",
       "  427,\n",
       "  329,\n",
       "  489,\n",
       "  43,\n",
       "  71,\n",
       "  211,\n",
       "  274,\n",
       "  1081,\n",
       "  95,\n",
       "  84,\n",
       "  108,\n",
       "  65,\n",
       "  112,\n",
       "  515,\n",
       "  1429,\n",
       "  573,\n",
       "  213,\n",
       "  87,\n",
       "  114,\n",
       "  235,\n",
       "  101,\n",
       "  492,\n",
       "  352,\n",
       "  154,\n",
       "  143,\n",
       "  247,\n",
       "  7,\n",
       "  2395,\n",
       "  242,\n",
       "  118,\n",
       "  452,\n",
       "  151,\n",
       "  119,\n",
       "  328],\n",
       " 'word complexity': [5,\n",
       "  5,\n",
       "  6,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  4,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  0,\n",
       "  6,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  4,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  5,\n",
       "  6,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  8,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  5,\n",
       "  6],\n",
       " 'sentence complexity': [23,\n",
       "  24,\n",
       "  25,\n",
       "  30,\n",
       "  30,\n",
       "  29,\n",
       "  22,\n",
       "  35,\n",
       "  30,\n",
       "  18,\n",
       "  44,\n",
       "  24,\n",
       "  28,\n",
       "  0,\n",
       "  35,\n",
       "  21,\n",
       "  33,\n",
       "  27,\n",
       "  24,\n",
       "  26,\n",
       "  29,\n",
       "  18,\n",
       "  29,\n",
       "  28,\n",
       "  39,\n",
       "  24,\n",
       "  13,\n",
       "  17,\n",
       "  31,\n",
       "  28,\n",
       "  28,\n",
       "  25,\n",
       "  27,\n",
       "  16,\n",
       "  25,\n",
       "  19,\n",
       "  20,\n",
       "  30,\n",
       "  31,\n",
       "  30,\n",
       "  24,\n",
       "  18,\n",
       "  23,\n",
       "  32,\n",
       "  34,\n",
       "  18,\n",
       "  22,\n",
       "  26,\n",
       "  13,\n",
       "  16,\n",
       "  25,\n",
       "  33,\n",
       "  30,\n",
       "  26,\n",
       "  28,\n",
       "  22,\n",
       "  41,\n",
       "  27,\n",
       "  30,\n",
       "  29,\n",
       "  27,\n",
       "  24,\n",
       "  24,\n",
       "  24,\n",
       "  23,\n",
       "  14,\n",
       "  32,\n",
       "  25,\n",
       "  50,\n",
       "  25,\n",
       "  37,\n",
       "  7,\n",
       "  20,\n",
       "  20,\n",
       "  41,\n",
       "  31,\n",
       "  34,\n",
       "  27,\n",
       "  38],\n",
       " 'unique words pct': [0.6367924528301887,\n",
       "  0.8333333333333334,\n",
       "  0.5894736842105263,\n",
       "  0.4303030303030303,\n",
       "  0.29793510324483774,\n",
       "  0.6169014084507042,\n",
       "  0.5879396984924623,\n",
       "  0.5076923076923077,\n",
       "  0.6956521739130435,\n",
       "  0.4962121212121212,\n",
       "  0.75,\n",
       "  0.3901147396293027,\n",
       "  0.526813880126183,\n",
       "  0,\n",
       "  0.5406032482598608,\n",
       "  0.7241379310344828,\n",
       "  0.3535528596187175,\n",
       "  0.2544094067343666,\n",
       "  0.6436781609195402,\n",
       "  0.2402948402948403,\n",
       "  0.2588336684860672,\n",
       "  0.5062166962699822,\n",
       "  0.49730700179533216,\n",
       "  0.5361305361305362,\n",
       "  0.6025641025641025,\n",
       "  0.6235294117647059,\n",
       "  0.7317073170731707,\n",
       "  0.7647058823529411,\n",
       "  0.4240437158469945,\n",
       "  0.6190476190476191,\n",
       "  0.7906976744186046,\n",
       "  0.4156219864995178,\n",
       "  0.54627539503386,\n",
       "  0.7128712871287128,\n",
       "  0.3574523396880416,\n",
       "  0.44951590594744123,\n",
       "  0.551487414187643,\n",
       "  0.7133333333333334,\n",
       "  0.5750798722044729,\n",
       "  0.6270270270270271,\n",
       "  0.40958408679927666,\n",
       "  0.5076530612244898,\n",
       "  0.5,\n",
       "  0.45147679324894513,\n",
       "  0.3209302325581395,\n",
       "  0.34105431309904155,\n",
       "  0.493993993993994,\n",
       "  0.30890713834491473,\n",
       "  0.8113207547169812,\n",
       "  0.7244897959183674,\n",
       "  0.5877437325905293,\n",
       "  0.5160075329566854,\n",
       "  0.24635369188696446,\n",
       "  0.7196969696969697,\n",
       "  0.7368421052631579,\n",
       "  0.7883211678832117,\n",
       "  0.7831325301204819,\n",
       "  0.5022421524663677,\n",
       "  0.41700404858299595,\n",
       "  0.22707770538693786,\n",
       "  0.3610586011342155,\n",
       "  0.41040462427745666,\n",
       "  0.725,\n",
       "  0.6785714285714286,\n",
       "  0.5365296803652968,\n",
       "  0.7013888888888888,\n",
       "  0.38954869358669836,\n",
       "  0.46194225721784776,\n",
       "  0.6086956521739131,\n",
       "  0.5017543859649123,\n",
       "  0.4137353433835846,\n",
       "  1.0,\n",
       "  0.20935314685314685,\n",
       "  0.4662813102119461,\n",
       "  0.7151515151515152,\n",
       "  0.3328424153166421,\n",
       "  0.6317991631799164,\n",
       "  0.6296296296296297,\n",
       "  0.4438430311231394]}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_dict_OCC['OCC-2018-0038']['Proposed Rule']['OCC-2018-0038-0001']['comment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output to JSON files for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json = json.dumps(res_dict_OCC)\n",
    "f = open(\"res_dict_OCC.json\",\"w\")\n",
    "f.write(json)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open(\"res_dict_OCC.json\",\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
